# -*- coding: utf-8 -*-
"""Stroke Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1APUZN7061SQzU01S7gxeY55vR9nHoaNW

##**Useful Library Import**
"""

import numpy as np
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings("ignore")
import torch 
import os
import tensorflow as tf
from tensorflow import keras
import torch.nn as nn
from torch.autograd import Variable
from sklearn.utils import shuffle
from torchsummary import summary
from sklearn.model_selection import GridSearchCV , train_test_split , StratifiedShuffleSplit , StratifiedKFold, cross_val_score , RepeatedStratifiedKFold, RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder , StandardScaler
from sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score,ConfusionMatrixDisplay,precision_score,recall_score,f1_score,classification_report,roc_curve,plot_roc_curve,auc,precision_recall_curve,plot_precision_recall_curve,average_precision_score

"""##**Useful models import**"""

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier 
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
import xgboost as xgb
import lightgbm as lgb

"""##**Reading Dataset**"""

df = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vQBkbBa7swoXVRNgWzDQDhAFZDp_MvcAPusdQkE7y_FcFVx0SjCvXY8uIbzmsbX6hvOWXL1AjLjzjDq/pub?output=csv')
df

"""##**Preprocessing of dataset**"""

df = df.drop(columns=['id'], axis = 1)
df.head()

"""Checking for missing values"""

df.isnull().sum()

"""Erasing missing values"""

df = df.dropna()
df.isnull().sum()

"""Checking the type of each columns"""

df.info()

"""Looking for the categorical columns which needs to be LabelEncoded"""

categorical_columns = []
for column in df.columns:
  if df[column].dtypes == object:
    categorical_columns.append(column)
categorical_columns

"""Let us keep initially the continuous value columns and discrete value columns in numerical category."""

numerical_columns = []
for column in df.columns:
  if column not in categorical_columns and column!='stroke':
    numerical_columns.append(column)
numerical_columns

"""#**Exploratory Data Analysis**

Checking is there exist any imbalance in the dataset or not
"""

labels = df['stroke'].value_counts(sort = True).index
sizes = df['stroke'].value_counts(sort = True)
colors = ["green","red"]
expd = (0.3,0) 
plt.figure(figsize=(5,5))
plt.pie(sizes,explode=expd,labels=labels,colors=colors,autopct='%1.1f%%',shadow=True,startangle=90)
plt.title('Number of stroke in the dataset')
plt.legend(["Not having stroke" , "Having stroke"])
plt.show()

"""From the above pie chart it is clearly visible that there is high imbalance in the dataset.

##Which metric to use?

*  Now, since the dataset is imbalanced and it is the binary classification problem, the best metric for evalution would be *Area under the ROC curve (AUC)*.
We can also use precesion and recall, but AUC combines both of these metrices.



*   Since it is a binary classification problem and from the above target distribution we can see that there are skewed targets hence we need to use *StratifiedKFold to split the data*

##Categorical Columns  --  [ ' gender ' , ' ever_married ' , ' work_type ' , ' Residence_type ' , ' smoking_status ' ]

For all the features lets see the variation of numbers against the target variable 'stroke'
"""

sns.countplot(x='gender', hue='stroke', data=df)

"""From the above graph, it seems that exist some samples for gender = 'Other' , Lets see how much the count is:"""

df['gender'].value_counts()

"""Since only a single value exist for gender = 'Other' we can easily drop this value"""

df = df.drop(df[df['gender'] == 'Other'].index,axis = 0)

df['gender'].value_counts()

sns.countplot(x='ever_married', hue='stroke', data=df)

"""Now it can be seen that the chance of getting stroke if married is larger than not married, so seems to be a useful feature in classification"""

sns.countplot(x='work_type', hue='stroke', data=df)

"""The people who work are likely to get stroke """

sns.countplot(x='Residence_type', hue='stroke', data=df)

"""Since the number for both the classes seems to be approximately equal, it doesnot sounds to be an important featture"""

sns.countplot(x='smoking_status', hue='stroke', data=df)

"""Wonder!!!
Smoking should be a important feature while classifying for the stroke but alas we found that only a very little difference is present among all, lets see the actual figures.
"""

df.groupby(['smoking_status','stroke'])['stroke'].count()

"""Lets keep this feature on the accout that it is more often to have stroke if the person smokes as (formerly smoke+ smokes > never smoked)

##Discrete Columns -- [ ' heart_disease' , ' hypertension ' ]
"""

sns.countplot(x='heart_disease', hue='stroke', data=df)

"""It seems like people having Heart Disease are more likely to have stroke"""

sns.countplot(x='hypertension', hue='stroke', data=df)

"""Let's see the original numbers"""

df['hypertension'].value_counts()

df.groupby(['hypertension','stroke'])['stroke'].count()

"""It seems like people not having any hypertension are likely to get stroke

Lets check the correctness of above statement with heart diesease as it is a fact that a person with hypertension is likely to stroke.
"""

cross_tab=pd.crosstab(df['heart_disease'],df['hypertension'],normalize='index')
sns.heatmap(cross_tab,annot=True,fmt="1.5%")
plt.show()

"""So from above heatmap, we can conclude that a person with *not* having hypertension is likely to have stroke irrespective of whether he/she has heart_disease or not.

##Continuous Columns -- [ ' age ' , ' avg_glucose_level ' , ' bmi ' ]
"""

sns.kdeplot(data=df[df['stroke']==0],x='age',color='green', shade=True,alpha=0.3)
sns.kdeplot(data=df[df['stroke']==1],x='age',color='red', shade=True,alpha=0.3)
plt.title("Distribution of age ",fontdict={'fontweight': 'bold', 'size':18})
plt.legend(['No stroke' , 'stroke'],loc = 'upper left')

"""From the above distribution, people with more age are likely to have stroke than the people with less age."""

sns.kdeplot(data=df[df['stroke']==0],x='avg_glucose_level',color='green', shade=True,alpha=0.3)
sns.kdeplot(data=df[df['stroke']==1],x='avg_glucose_level',color='red', shade=True,alpha=0.3)
plt.title("Distribution of avg_glucose_level",fontdict={'fontweight': 'bold', 'size':18})
plt.legend(['No stroke' , 'stroke'],loc = 'upper right')

"""There is *very little relation between avg_glucose_level and stroke*. Similar glucose level has stroke as well as not stroke. If a person has avg_glucose_level > 150, he/she has very likely to have stroke."""

sns.kdeplot(data=df[df['stroke']==0],x='bmi',color='green', shade=True,alpha=0.3)
sns.kdeplot(data=df[df['stroke']==1],x='bmi',color='red', shade=True,alpha=0.3)
plt.title("Distribution of bmi ",fontdict={'fontweight': 'bold', 'size':18})
plt.legend(['No stroke' , 'stroke'],loc = 'upper right')

"""There is *no relation between bmi and stroke*. Similar bmi has stroke as well as not stroke.

#**Feature Engineering**

## Encoding Categorical columns
"""

print(categorical_columns)

for column in categorical_columns:
  print(column + " has "  + str(df[column].unique()) + " unique values")

encode = {"gender" : { "Male" : 1 , "Female" : 0} , "ever_married" : {"Yes" : 1 , "No" : 0} , "work_type" : {"Private" : 0 , "Self-employed" : 1 , 
                "Govt_job" : 2 , "children" : 3, "Never_worked" : 4} , "Residence_type" : {"Urban" : 1 , "Rural" : 0} , "smoking_status" : {"formerly smoked" : 0
                , "never smoked" : 1, "smokes" : 2 , "Unknown" : 3}}
df = df.replace(encode)
df

"""## Dealing with continuous columns"""

continuous_columns = ['age','avg_glucose_level','bmi']
df[continuous_columns].describe()

"""As we can see that continuous columns are measured in differnt scales so we need to standarize them by doing standardization (Standard scaler)."""

scaler = StandardScaler()
df[continuous_columns] = scaler.fit_transform(df[continuous_columns])
df

"""##**Splitting data in training(70%), validation(10%) and testing(20%) using stratified split method**"""

df.reset_index(inplace = True)
df.drop("index",axis = 1, inplace = True)

data = df.copy()
data = data.drop(columns = ['bmi'],axis=1)
X = data.drop(columns=['stroke'],axis=1)
Y = data[['stroke']]

def split(X,Y):
  sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3,random_state = 42)
  for train_index, test_index in sss.split(X, Y):
    x_train = X.loc[train_index]
    y_train = Y.loc[train_index]
    test_x = X.loc[test_index]
    test_y = Y.loc[test_index]

  x_train.reset_index(inplace = True)
  x_train.drop("index",axis = 1, inplace = True)

  y_train.reset_index(inplace = True)
  y_train.drop("index",axis = 1, inplace = True)

  test_x.reset_index(inplace = True)
  test_x.drop("index",axis = 1, inplace = True)

  test_y.reset_index(inplace = True)
  test_y.drop("index",axis = 1, inplace = True)

  sss = StratifiedShuffleSplit(n_splits=1, test_size= 1/3, random_state=42)
  for test_index, val_index in sss.split(test_x, test_y):
    x_test = test_x.loc[test_index]
    y_test = test_y.loc[test_index]
    x_val =   test_x.loc[val_index]
    y_val =   test_y.loc[val_index]


  x_test.reset_index(inplace = True)
  x_test.drop("index",axis = 1, inplace = True)

  y_test.reset_index(inplace = True)
  y_test.drop("index",axis = 1, inplace = True)

  x_val.reset_index(inplace = True)
  x_val.drop("index",axis = 1, inplace = True)

  y_val.reset_index(inplace = True)
  y_val.drop("index",axis = 1, inplace = True)

  return x_train, y_train , x_val, y_val , x_test , y_test

x_train, y_train , x_val, y_val , x_test , y_test = split(X,Y)

"""#**Training multiple models without taking care of imbalanced dataset**"""

models = [LogisticRegression() , KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(), BernoulliNB() , GradientBoostingClassifier()]
model_names = ['LogisticRegression','KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier','BernoulliNB','GradientBoostingClassifier','XGBClassifier','LightGBM']
Precision = []
Recall = []
ROC = []
F1 = []

def training_summary(x_train, y_train, x_val, y_val, model_name , model):
  model.fit(x_train, y_train)
  prediction = model.predict(x_val)
  precision = precision_score(y_val, prediction)*100
  recall = recall_score(y_val, prediction)*100
  roc = roc_auc_score(y_val,prediction)*100
  f1 = f1_score(y_val,prediction)*100
  Precision.append(precision)
  Recall.append(recall)
  ROC.append(roc)
  F1.append(f1)
  # return precision, recall , roc, f1

def modelling():
  training_summary(x_train,y_train, x_val, y_val, 'LogisticRegression',LogisticRegression())
  training_summary(x_train,y_train, x_val, y_val, 'KNeighborsClassifier',KNeighborsClassifier(n_neighbors=1,algorithm='kd_tree',weights='uniform'))
  training_summary(x_train,y_train, x_val, y_val, 'DecisionTreeClassifier',DecisionTreeClassifier())
  training_summary(x_train,y_train, x_val, y_val, 'RandomForestClassifier',RandomForestClassifier(n_estimators=50,n_jobs=-1))
  training_summary(x_train,y_train, x_val, y_val, 'BernoulliNB',BernoulliNB())
  training_summary(x_train,y_train, x_val, y_val, 'GradientBoostingClassifier',GradientBoostingClassifier(learning_rate=0.1,loss='exponential',max_depth=70,
                          max_features=2,n_estimators=300))
  training_summary(x_train,y_train, x_val, y_val, 'XGBClassifier',xgb.XGBClassifier())
  training_summary(x_train,y_train, x_val, y_val, 'LGBMClassifier',lgb.LGBMClassifier())

modelling()

model_names = ['LogisticRegression','KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier','BernoulliNB','GradientBoostingClassifier','XGBClassifier','LightGBM']
score = pd.DataFrame({'Model': model_names, 'Precision': Precision, 'Recall': Recall, 'ROC_AUC_Score' : ROC,'F1_score' : F1})
score.style.background_gradient(high=1,axis=0)

"""We can clearly see that how poor our models works as the dataset was highly imbalanced.

Let's firstly work on making the dataset balanced

#**Synthetic Minority Oversampling Technique**

<p align = center>This problem has imbalanced classification i.e., there are too few examples of the minority class for a model to effectively learn the decision boundary.One way to solve this problem is to oversample the examples in the minority class. This can be achieved by simply duplicating examples from the minority class in the training dataset prior to fitting a model. This can balance the class distribution but does not provide any additional information to the model</p>
<img src="https://miro.medium.com/max/1400/1*yRumRhn89acByodBz0H7oA.png">
<p >Synthetic Sample generation using SMOTE</p>

#**Defining object of SMOTE() and again using stratified split**
"""

smote = SMOTE()
X_smote , Y_smote = smote.fit_resample(X,Y)
x_train, y_train , x_val, y_val , x_test , y_test = split(X_smote,Y_smote)

data_smote = pd.concat([X_smote, Y_smote],axis=1)

"""## Visualising our SMOTE dataset"""

fig,ax=plt.subplots(1,2,figsize=(12,5))
sns.scatterplot(data=data,x='age',y='avg_glucose_level',hue='stroke',ax=ax[0])\
.set_title("Actual")
sns.scatterplot(data=data_smote,x='age',y='avg_glucose_level',hue='stroke',ax=ax[1])\
.set_title("SMOTE")

Precision = []
Recall = []
ROC = []
F1 = []

"""##**Modelling via SMOTE**"""

modelling()

"""**Scores after applying SMOTE**"""

model_names = ['LogisticRegression','KNeighborsClassifier','DecisionTreeClassifier','RandomForestClassifier','BernoulliNB','GradientBoostingClassifier','XGBClassifier','LightGBM']
score = pd.DataFrame({'Model': model_names, 'Precision': Precision, 'Recall': Recall,'ROC_AUC_Score' : ROC, 'F1_Score' : F1})
score.style.background_gradient(high=1,axis=0)

"""From the above results we can see that LogisticRegression , BernoulliNB didn't worked well as we can easily see them using ROC_AUC_score

Visualising the ROC_AUC_SCORE and see which is greatest and most accurate to use.
"""

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "ROC_AUC_Score" , data = score)
plt.title("Model Comparision based on ROC_AUC_Score	");

"""
Visualising the F1 score and see which is greatest and most accurate to use."""

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "F1_Score" , data = score)
plt.title("Model Comparision based on F1_Score");

"""Combining all the metrics used and looking for the correct models"""

plt.figure(figsize = (10 , 5))
plt.plot(model_names, F1, color = 'r', label = 'F1_score')
plt.plot(model_names, ROC, color = 'g', label = 'AUC_ROC_Score')
plt.plot(model_names, Precision, color = 'b', label = 'Precision')
plt.plot(model_names, Recall, color = 'y', label = 'Recall')
plt.xticks(rotation=90)
plt.xlabel("Models")
plt.ylabel("Score")
plt.legend()

"""Since If Stroke is not predicted correctly it may lead to severe damages to body and even death and it should be predicted accuractely, which implies if a person has a Stroke then it should be predicted that he has stroke rather than predicting not having stroke which implies False negative should be extremely small which means our model should be sensitive or we should take the model having high Recall Score.

From above clarification we can judge from the above graph that respective models works very good.


*   KNN
*   RFC
*   Gradient Boosting Classifier
*   XGBClassifier
*   LightGBMClassifier
*   DTC

Now's Let's Tune each one of these models to get the best hyperparameters so that we can obtain maximum AUC_ROC_score
"""

tuned_validation_roc_auc_scores = []

"""##Tuning XGB as we have got less ROC_AUC score for it"""

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ,9],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
}

xgb_ = XGBClassifier()
xgb_tuning = RandomizedSearchCV(xgb_, param_distributions = params, n_iter = 8, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)
xgb_tuning.fit(x_train , y_train)
xgb_tuning.best_estimator_
xgb_tuning.best_params_

tuned_validation_roc_auc_scores.append(max(0,roc_auc_score(y_val, xgb_tuning.predict(x_val))))

cv_score_xgb = max(0,roc_auc_score(y_val, xgb_tuning.predict(x_val)))

tuned_validation_roc_auc_scores

"""##Tuning LGBM Classifier"""

lgb_ = lgb.LGBMClassifier()
lgb_tuning = RandomizedSearchCV(lgb_, param_distributions = params, n_iter = 8, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)
lgb_tuning.fit(x_train , y_train)
lgb_tuning.best_estimator_
lgb_tuning.best_params_
max(0,roc_auc_score(y_val, lgb_tuning.predict(x_val)))

lgb_tuning.best_params_

cv_score_lgbm = max(0,roc_auc_score(y_val, lgb_tuning.predict(x_val)))

"""##Tuning Random Forest Classifier"""

param_dist = {
'n_estimators': list(range(10, 300, 5)),
'min_samples_leaf': list(range(1, 50)),
'max_depth': list(range(2, 30)),
'max_features': ['auto', 'sqrt'],
'bootstrap': [True, False]
}
rfc = RandomForestClassifier()
rfc_tuning = RandomizedSearchCV(rfc, param_distributions = param_dist, n_iter = 8, scoring = 'roc_auc', n_jobs = 1, cv = 5, verbose = 3)
rfc_tuning.fit(x_train , y_train)
rfc_tuning.best_estimator_
rfc_tuning.best_params_
max(0,roc_auc_score(y_val, rfc_tuning.predict(x_val)))

rfc_tuning.best_params_

"""##n-fold cross validation on the selected models"""

def cross_validation(model, train_x, train_y, folds):
  mdl = model
  cv = RepeatedStratifiedKFold(n_splits= folds, n_repeats=1, random_state=42)
  scores_precision = cross_val_score(mdl, train_x, train_y, cv = cv, scoring = "precision")
  scores_recall    = cross_val_score(mdl, train_x, train_y, cv = cv, scoring = "recall")
  scores_roc_auc   = cross_val_score(mdl, train_x, train_y, cv = cv, scoring = "roc_auc")
  return scores_precision, scores_recall , scores_roc_auc

train_x = pd.concat([x_train,x_val], axis = 0)
train_y = pd.concat([y_train,y_val], axis = 0)
folds = list(range(0,10))
cv_scores_precision_rfc, cv_scores_recall_rfc , cv_scores_roc_auc_rfc = cross_validation(RandomForestClassifier(n_estimators=50,n_jobs=-1), train_x, train_y,len(folds))
cv_scores_precision_knn, cv_scores_recall_knn , cv_scores_roc_auc_knn = cross_validation(KNeighborsClassifier(n_neighbors=1,algorithm='kd_tree',weights='uniform'), train_x, train_y,len(folds))
cv_scores_precision_dtc, cv_scores_recall_dtc , cv_scores_roc_auc_dtc = cross_validation(DecisionTreeClassifier(), train_x, train_y,len(folds))
cv_scores_precision_gbc, cv_scores_recall_gbc , cv_scores_roc_auc_gbc = cross_validation(GradientBoostingClassifier(learning_rate=0.1,loss='exponential',max_depth=70,
                          max_features=2,n_estimators=300), train_x, train_y,len(folds))


def cross_validation_plotting(model_name,score,folds,scoring):
  plt.plot(folds,score)
  plt.xlabel("Fold number")
  plt.ylabel("Score of respective fold")
  plt.title("Cross Validation score on " + model_name + " based on " + scoring ,fontdict={'fontweight': 'bold', 'size':18})
  plt.show()
  print()
  print("Mean Cross Validation scores based on " + scoring + "  is : " + str(sum(score)/len(score)))
  print()

"""Generating the cross validation results for each model and visualising each metric for each model

###RFC
"""

cross_validation_plotting("RFC",cv_scores_precision_rfc,folds,"Precision")
cross_validation_plotting("RFC",cv_scores_recall_rfc,folds,"Recall")
cross_validation_plotting("RFC",cv_scores_roc_auc_rfc,folds,"ROC_AUC")

"""###KNN"""

cross_validation_plotting("KNN",cv_scores_precision_knn,folds,"Precision")
cross_validation_plotting("KNN",cv_scores_recall_knn,folds,"Recall")
cross_validation_plotting("KNN",cv_scores_roc_auc_knn,folds,"ROC_AUC")

"""###DTC"""

cross_validation_plotting("DTC",cv_scores_precision_dtc,folds,"Precision")
cross_validation_plotting("DTC",cv_scores_recall_dtc,folds,"Recall")
cross_validation_plotting("DTC",cv_scores_roc_auc_dtc,folds,"ROC_AUC")

"""###GBC"""

cross_validation_plotting("GBC",cv_scores_precision_gbc,folds,"Precision")
cross_validation_plotting("GBC",cv_scores_recall_gbc,folds,"Recall")
cross_validation_plotting("GBC",cv_scores_roc_auc_gbc,folds,"ROC_AUC")

"""###Selecting on the basis of ROC AUC Score"""

cv_auc_roc_scores = []
cv_auc_roc_scores.append(sum(cv_scores_roc_auc_rfc)/ len(cv_scores_roc_auc_rfc))
cv_auc_roc_scores.append(sum(cv_scores_roc_auc_knn)/ len(cv_scores_roc_auc_knn))
cv_auc_roc_scores.append(sum(cv_scores_roc_auc_dtc)/ len(cv_scores_roc_auc_dtc))
cv_auc_roc_scores.append(sum(cv_scores_roc_auc_gbc)/ len(cv_scores_roc_auc_gbc))
cv_auc_roc_scores.append(cv_score_xgb)
cv_auc_roc_scores.append(cv_score_lgbm)

model_names = ['RandomForestClassifier', 'KNeighborsClassifier','DecisionTreeClassifier','GradientBoostingClassifier','XGBClassifier','LightGBM']
score = pd.DataFrame({'Model': model_names, 'Cross Validation AUC ROC Score':cv_auc_roc_scores})
score.style.background_gradient(high=1,axis=0)

"""So from above models we finally take RFC, GBC as classifier apart from xgb and lgbm

##Training a neural netwrok
"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

def get_accuracy(logit, target, batch_size):
    corrects = (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()
    accuracy = 100.0 * corrects/batch_size
    return accuracy.item()

x_train_nn=x_train.values
y_train_nn=y_train.values

"""Defining Neural Network parameters"""

batch_size = 100
num_epochs = 100
learning_rate = 0.1
size_hidden_1 = 200
size_hidden_2 = 200
num_classes = 10
batch_no = len(x_train_nn) // batch_size 
cols = 8

"""Making neural network class"""

class Net(torch.nn.Module):
    def __init__(self, num_inputs, size_hidden_1, size_hidden_2, n_output, activation_function):
        super(Net, self).__init__()

        self.flatten1                     = torch.nn.Flatten()
        
        self.dense1                       = torch.nn.Linear(num_inputs,size_hidden_1)
        self.activi1                      = activation_function
        self.balancenormal1               = torch.nn.BatchNorm1d(size_hidden_1)
        
        self.dense2                       = torch.nn.Linear(size_hidden_1,size_hidden_1)
        self.activi2                      = activation_function
        self.balancenormal2               = torch.nn.BatchNorm1d(size_hidden_1)
        
        self.dense3                       = torch.nn.Linear(size_hidden_1,size_hidden_1)
        self.activi3                      = activation_function
        self.balancenormal3               = torch.nn.BatchNorm1d(size_hidden_1)
        
        self.dense4                       = torch.nn.Linear(size_hidden_1,size_hidden_1)
        self.activi4                      = activation_function
        self.balancenormal4               = torch.nn.BatchNorm1d(size_hidden_1)

        self.dense5                       = torch.nn.Linear(size_hidden_1,size_hidden_1)
        self.activi5                      = activation_function
        self.balancenormal5               = torch.nn.BatchNorm1d(size_hidden_1)

        self.dense6                       = torch.nn.Linear(size_hidden_1,size_hidden_2)
        self.activi6                      = activation_function
        self.balancenormal6               = torch.nn.BatchNorm1d(size_hidden_2)

        self.dense7                      = torch.nn.Linear(size_hidden_2,size_hidden_2)
        self.activi7                      = activation_function
        self.balancenormal7               = torch.nn.BatchNorm1d(size_hidden_2)

        self.denseoutput                  = torch.nn.Linear(size_hidden_2,n_output)
        self.activipre                    = activation_function
        self.activioutput                 = torch.nn.Softmax(dim=1)

    def forward(self, x):
        a= self.flatten1(x)
        a= self.dense1(self.activi1(a))
        a= self.balancenormal1(a)
        a= self.dense2(self.activi2(a))
        a= self.balancenormal2(a)
        a= self.dense3(self.activi3(a))
        a= self.balancenormal3(a)
        a= self.dense4(self.activi4(a))
        a= self.balancenormal4(a)
        a= self.dense5(self.activi5(a))
        a= self.balancenormal5(a)
        a= self.dense6(self.activi6(a))
        a= self.balancenormal6(a)
        a= self.dense7(self.activi7(a))
        a= self.balancenormal7(a)
        a= self.denseoutput(self.activipre(a))
        a= self.activioutput(a)
        return(a)

activation_function=torch.nn.ReLU()
net = Net(9, 256, 256, 2, activation_function)

"""Training neural network"""

def train_neural_network(model,x,y,size_hidden_1,size_hidden_2,num_epochs,lr,batch_size):
    batch_no=len(x.values)// batch_size
    x=x.values
    y=y.values
    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
    loss_func = torch.nn.CrossEntropyLoss()  
    for epoch in range(num_epochs):
        x, y = shuffle(x, y)
        train_acc = 0.0
        running_loss = 0.0

        for i in range(batch_no):
            start = i * batch_size
            end = start + batch_size
            inputs = Variable(torch.FloatTensor(x[start:end]))
            labels = Variable(torch.LongTensor(y[start:end]))
            
            optimizer.zero_grad()
            outputs = model(inputs)
            
            loss = loss_func(outputs, labels)
            
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            acc = get_accuracy(outputs, labels, batch_size)
            train_acc += acc
          
          # print('Epoch: %d | Loss: %.4f | Train Accuracy: %.2f' \
          #       %(epoch+1, running_loss / (i+1), train_acc/(i+1)))  
        running_loss = 0.0
    return(model)

train_neural_network(net,x_train,y_train['stroke'],size_hidden_1,size_hidden_2,num_epochs,learning_rate,batch_size)

"""Testing neural network"""

def testing_nn(model,x,y,batch_size):
    x=x.values
    y=y['stroke'].values
    inputs=Variable(torch.FloatTensor(x))
    labels=Variable(torch.FloatTensor(y))
    outputs= model(inputs)
    return(get_accuracy(outputs, labels, batch_size))

def prediction_nn(model,inputs,labels, batch_size):
    outputs=model(inputs)
    # return(outputs)
    return(get_accuracy(outputs,labels, batch_size))

x_nn_val=x_val.values
y_nn_val=y_val.values

x_nn_test=x_test.values
y_nn_test=y_test.values

m=len(x_nn_val)
input_test= Variable(torch.FloatTensor(x_nn_val[0:m]))
label_test= Variable(torch.LongTensor(y_nn_val[0:m]))

y_nn_pred=net(input_test)
validation_accuracy = get_accuracy(y_nn_pred, label_test, m)
validation_accuracy

"""Since the trained neural network is not as effective as the in built classifier hence we would be droppping the neural network for the final classification

#**Pipeline**
<img src="https://github.com/Shrut26/Stroke-Prediction/blob/main/Stroke%20Pipeline.png?raw=true" />
"""

class Pipeline():
  def __init__(self,X,Y):
    smote = SMOTE()
    X_smote , Y_smote = smote.fit_resample(X,Y)
    x_train, y_train , x_val, y_val , x_test , y_test = self.split(X_smote,Y_smote)
    self.x_train = x_train
    self.y_train = y_train
    self.x_test  = x_test
    self.y_test  = y_test

  def split(self,X,Y):
    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3,random_state = 42)
    for train_index, test_index in sss.split(X, Y):
      x_train = X.loc[train_index]
      y_train = Y.loc[train_index]
      test_x = X.loc[test_index]
      test_y = Y.loc[test_index]

    x_train.reset_index(inplace = True)
    x_train.drop("index",axis = 1, inplace = True)

    y_train.reset_index(inplace = True)
    y_train.drop("index",axis = 1, inplace = True)

    test_x.reset_index(inplace = True)
    test_x.drop("index",axis = 1, inplace = True)

    test_y.reset_index(inplace = True)
    test_y.drop("index",axis = 1, inplace = True)

    sss = StratifiedShuffleSplit(n_splits=1, test_size= 1/3, random_state=42)
    for test_index, val_index in sss.split(test_x, test_y):
      x_test = test_x.loc[test_index]
      y_test = test_y.loc[test_index]
      x_val =   test_x.loc[val_index]
      y_val =   test_y.loc[val_index]


    x_test.reset_index(inplace = True)
    x_test.drop("index",axis = 1, inplace = True)

    y_test.reset_index(inplace = True)
    y_test.drop("index",axis = 1, inplace = True)

    x_val.reset_index(inplace = True)
    x_val.drop("index",axis = 1, inplace = True)

    y_val.reset_index(inplace = True)
    y_val.drop("index",axis = 1, inplace = True)

    return x_train, y_train , x_val, y_val , x_test , y_test
 

  def xgbclassification(self):
    xgbclassifier = XGBClassifier(colsample_bytree = 0.7,gamma = 0.1,learning_rate =  0.1,max_depth = 8, min_child_weight = 3)
    xgbclassifier.fit(self.x_train, self.y_train)
    xgb_pred = xgbclassifier.predict(self.x_test)
    score = max(0,100*roc_auc_score(self.y_test, xgb_pred))
    print("Individual classifcation accuracy taking XGBClassifier is : " + str(score))
    return xgb_pred

  #lgbm
  def lgbmclassification(self):
    lgbmclassifier = lgb.LGBMClassifier(colsample_bytree = 0.7,gamma = 0.1,learning_rate = 0.3,max_depth= 15,min_child_weight = 5)
    lgbmclassifier.fit(self.x_train, self.y_train)
    lgbm_pred = lgbmclassifier.predict(self.x_test)
    score = max(0,100*roc_auc_score(self.y_test, lgbm_pred))
    print("Individual classifcation accuracy taking LGBBClassifier is : " + str(score))
    return lgbm_pred

  #gbc
  def gbcclassification(self):
    gbc = GradientBoostingClassifier(learning_rate=0.1,loss='exponential',max_depth=70,max_features=2,n_estimators=300)
    gbc.fit(self.x_train, self.y_train)
    gbc_pred = gbc.predict(self.x_test)
    score = max(0,100*roc_auc_score(self.y_test, gbc_pred))
    print("Individual classifcation accuracy taking Gradient Boosting Classifier is : " + str(score))
    return gbc_pred
  #rfc
  
  def rfcclassification(self):
    rfc = RandomForestClassifier(bootstrap = False, max_depth = 24,max_features = 'sqrt',min_samples_leaf = 3, n_estimators = 265)
    # rfc = RandomForestClassifier(n_estimators=50,n_jobs=-1)
    rfc.fit(self.x_train, self.y_train)
    rfc_pred = rfc.predict(self.x_test)
    score = max(0,100*roc_auc_score(self.y_test, rfc_pred))
    print("Individual classifcation accuracy taking Random Forest Classifier is : " + str(score))
    return rfc_pred

  def prediction(self,xgb_pred,lgbm_pred,gbc_pred,rfc_pred):
    final_prediction = []

    for iter in range(len(xgb_pred)):
      i = xgb_pred[iter]
      j = lgbm_pred[iter]
      k = gbc_pred[iter]
      l = rfc_pred[iter]


      zero = 0
      ones = 0

      if(i == 0):
        zero+=1
      elif(i==1):
        ones+=1

      if(j == 0):
        zero+=1
      elif(j==1):
        ones+=1

      if(k == 0):
        zero+=1
      elif(k==1):
        ones+=1

      if(l== 0):
        zero+=1
      elif(l==1):
        ones+=1

    
      if(zero > ones):
        final_prediction.append(0)
      
      if(ones > zero):
        final_prediction.append(1)

      if(zero == ones):
        if(i == 0):
          final_prediction.append(0)
        else:
          final_prediction.append(1)
      
    return final_prediction
      
  def Scoring(self,final_prediction):
    score = max(0,100*roc_auc_score(self.y_test, final_prediction))
    return score

pip = Pipeline(X,Y)
xgb_pred = pip.xgbclassification()
lgbm_pred = pip.lgbmclassification()
gbc_pred= pip.gbcclassification()
rfc_pred = pip.rfcclassification()
predictions = pip.prediction(xgb_pred,lgbm_pred,gbc_pred,rfc_pred)
score = pip.Scoring(predictions)
print("THE FINAL TESTING SCORE RECEIVED FOR THE GIVEN DATASET TAKING MAJORITY VOTING: " + str(score))

"""<p align = center>We can clearly see that Majority voting is performing better than using individual classifiers.</p>"""